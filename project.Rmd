---
title: "HumanActivityRecognition"
author: "Bryan Wolfford"
date: "Friday, February 20, 2015"
output: html_document
---

##Inspiration
Human activity recognition research has traditionally focused on discriminating between different activities, however, I propose an investigation into "how (well)" an activity was performed.

##Question
Can quantitaive characteristics be used to classify if an exercise was performed correctly?

##Input Data
Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).

Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes. Participants were supervised by an experienced weight lifter to make sure the execution complied to the manner they were supposed to simulate. The exercises were performed by six male participants aged between 20-28 years, with little weight lifting experience. We made sure that all participants could easily simulate the mistakes in a safe and controlled manner by using a relatively light dumbbell (1.25kg).

Read more: (http://groupware.les.inf.puc-rio.br/har#ixzz3SRC5ymYf)[http://groupware.les.inf.puc-rio.br/har#ixzz3SRC5ymYf]

```{r}
train  <- read.csv("pml-training.csv", header=TRUE, na.strings = c("", " ","NA"))
test  <- read.csv("pml-testing.csv", header=TRUE, na.strings = c("", " ","NA"))
```

##Features
For feature extraction we used a sliding window approach with different lengths from 0.5 second to 2.5 seconds, with 0.5 second overlap. In each step of the sliding window approach we calculated features on the Euler angles (roll, pitch and yaw), as well as the raw accelerometer, gyroscope and magnetometer readings. For the Euler angles of each of the four sensors we calculated eight features: mean, variance, standard deviation, max, min, amplitude, kurtosis and skewness, generating in total 96 derived feature sets.

For training, we then took that WLE dataset and removed the first 7 columns because they are all identifiers and not sensor readings, which if included in the training, would either have no effect, or worse, may bias the model.

Next Analysis of missing data was performed, and features missing data were removed before training the model. Credit for function, nacols, goes to (StackOverflow)[https://class.coursera.org/predmachlearn-011/forum/thread?thread_id=118]. Removal of NAs was chosen for algorymthmic simplicity, however, imputing of missing data would be performed in the case that an accuracy of at least 95% was not achieved.

```{r}
train <- train[,8:length(train)]
test <- test[,8:length(test)]

nacols <- function(df) {
    colnames(df)[unlist(lapply(df, function(x) any(is.na(x))))]
}
removeNAs <- nacols(train)

train <- train[, -which(names(train) %in% removeNAs)]
test <- test[, -which(names(test) %in% removeNAs)]
```

##Algorithm
Even though the provided data was already seperated for training and testing, the test size is only 20 which is inappropriate, so for further cross validation, we first slice the training data set into 60% for training, and 40% for model testing.

### How you built your model?
**For reproducibility, first set the seed!** With that 60% training data, we then use a Random Forest approach for the classification model becuase of its well known robustness, accuracy, and typically low out-of-sample error.
```{r}
library(caret); library(randomForest)
set.seed(80085)
inTrain <- createDataPartition(y=train$classe, p=.6, list=FALSE)
myTrain <- train[inTrain,]
myTest <- train[-inTrain,]

Training <- 1:length(train$classe)
Training[inTrain] <- TRUE
Training[-inTrain] <- FALSE
qplot(classe, 1:length(train$classe), data=train, color=Training, main="Classification Data for Training and Testing", xlab="Sample Classification", ylab="Data Index")
```

```{r}
rf <- randomForest(classe ~ ., data = myTrain)
plot(rf, main ="Random Forest Error by number of Trees in model.")
```

As evident by the plot, optimum accuracy is achieved closer to 100 trees and not 500, so our model could be optimized be reducing nubmer of trees used.

##Evaluation
To evalute our model, we predict the actual classes for each example of the provided test as well as for our 40% test data.

### How you used cross validation?
Cross validating our preditions for the 40% data, results in both an Accuracy and Kappa value of 99%.

Also using our model for prediction, we achieved 100% accuracy for the 20 examples in the provided test data. 

```{r}
pred <- predict(rf, train)
myPred <- predict(rf, myTest)

cm <- confusionMatrix(myPred, myTest$classe)
cm$table
cm$overall
```

### What do you think is the expected out of sample error?
The above analysis corresponds with the out-of-sample error at 1%

Credit for the WLE data goes to:
(Groupware@LES ilustração, seta para a direita Projects ilustração, seta para a direita Human Activity Recognition)[http://groupware.les.inf.puc-rio.br/har#weight_lifting_exercises]
Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.